{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61b43ebb-b1d3-4ebd-bf5a-03d7fa927211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7482b0-5591-416b-a3fc-cdf748bf221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Time-dependent seasonal components refer to patterns or variations in data that occur periodically over time, with the characteristics of these patterns changing over different time periods. These components are typically observed in time series data and are influenced by recurring events or factors that follow a seasonal or cyclical pattern.\n",
    "\n",
    "To better understand time-dependent seasonal components, let's consider an example. Suppose we have a dataset that records the daily sales of ice cream in a beach town over several years. The data would consist of a timestamp (date) and the corresponding sales value for each day.\n",
    "\n",
    "Upon analyzing this dataset, we might observe certain seasonal patterns that repeat each year. For example, we may notice that the sales of ice cream are generally higher during the summer months compared to the rest of the year. Within each summer season, we may also observe smaller cyclical patterns, such as increased sales during weekends or holidays.\n",
    "\n",
    "These patterns can be considered time-dependent seasonal components. They exhibit a periodic nature that occurs within specific time intervals (e.g., yearly or within a particular season) and can be attributed to factors like weather conditions, tourist influx, or vacation periods.\n",
    "\n",
    "By identifying and modeling these time-dependent seasonal components, we can gain insights into the underlying patterns and make more accurate predictions or forecasts. This can be particularly useful for businesses to optimize inventory management, marketing strategies, and resource allocation based on seasonal fluctuations.\n",
    "\n",
    "In summary, time-dependent seasonal components represent recurring patterns or variations in data that occur periodically over time, with the characteristics of these patterns changing across different time periods. They provide valuable information about seasonal trends and can be leveraged for forecasting and decision-making purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcc1a2d9-b82d-4149-b7dd-81abfca36623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac05ae3-f36f-4a4b-8760-f60eca92f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "To identify time-dependent seasonal components in time series data, you can use various techniques such as visual inspection, autocorrelation analysis, and time series decomposition. Here's an overview of each method:\n",
    "\n",
    "1. Visual Inspection:\n",
    "   - Plot the time series data and examine it for recurring patterns or cycles. Look for regular fluctuations that occur at fixed intervals, such as daily, weekly, monthly, or yearly patterns.\n",
    "\n",
    "2. Autocorrelation Analysis:\n",
    "   - Autocorrelation measures the correlation between a time series and a lagged version of itself. By calculating the autocorrelation function (ACF) or partial autocorrelation function (PACF) of the time series, you can identify significant lags that indicate seasonal patterns.\n",
    "   - In Python, you can use the `statsmodels` library to calculate autocorrelation functions. The `plot_acf` and `plot_pacf` functions can help visualize the autocorrelation and partial autocorrelation plots, respectively.\n",
    "\n",
    "3. Time Series Decomposition:\n",
    "   - Decomposition separates a time series into its constituent components: trend, seasonal, and residual (or error) components.\n",
    "   - The decomposition methods commonly used are:\n",
    "     - Additive decomposition: Assumes that the seasonal component has a constant magnitude throughout the time series.\n",
    "     - Multiplicative decomposition: Assumes that the seasonal component's magnitude varies with the level of the time series.\n",
    "   - Python libraries like `statsmodels` and `seasonal` offer functions to perform time series decomposition. The resulting seasonal component can be analyzed and visualized.\n",
    "\n",
    "It's important to note that identifying time-dependent seasonal components may require a combination of these methods, and the choice of approach depends on the characteristics of your data and the nature of the seasonality you expect to find. Additionally, domain knowledge and further analysis might be needed to validate and interpret the identified seasonal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330cf386-df21-4adb-875c-c01e1d8bd083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load your time series data into a pandas DataFrame\n",
    "# Assuming your data has two columns: 'date' and 'value'\n",
    "data = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Convert the 'date' column to datetime type\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Set the 'date' column as the DataFrame index\n",
    "data.set_index('date', inplace=True)\n",
    "\n",
    "# Perform seasonal decomposition using the additive model\n",
    "decomposition = sm.tsa.seasonal_decompose(data['value'], model='additive')\n",
    "\n",
    "# Retrieve the seasonal component from the decomposition\n",
    "seasonal_component = decomposition.seasonal\n",
    "\n",
    "# Plot the seasonal component\n",
    "seasonal_component.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32b8b9-a62a-4a38-8ade-186ed686dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe6d8f-7696-450c-86fb-cf7d8e452db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Time-dependent seasonal components in time series data can be influenced by various factors. Here are some key factors that can affect the presence and characteristics of seasonal patterns:\n",
    "\n",
    "1. Nature of the Phenomenon: The underlying nature of the phenomenon being observed can significantly impact the presence and type of seasonal patterns. For example, in retail sales data, you might expect to see strong seasonal variations due to holidays and shopping seasons. In weather data, seasonal patterns can be influenced by changes in temperature or precipitation levels throughout the year.\n",
    "\n",
    "2. Human Behavior and Culture: Human behavior, cultural events, and societal factors can introduce seasonal patterns in data. For instance, consumer behavior during holiday seasons, festivals, or vacation periods can lead to regular patterns in sales, travel, or other economic indicators.\n",
    "\n",
    "3. Calendar Effects: The structure of the calendar, including the presence of holidays, weekends, and special events, can introduce seasonal components. For example, weekly patterns might arise due to variations in behavior between weekdays and weekends, while annual patterns might be influenced by holidays falling on specific dates.\n",
    "\n",
    "4. Climate and Weather: Time series related to climate and weather often exhibit strong seasonal components. The change in weather conditions across different seasons can lead to predictable variations, such as temperature fluctuations, precipitation patterns, or daylight hours.\n",
    "\n",
    "5. Business and Industry Factors: Industry-specific factors can introduce seasonal components. For instance, the demand for certain products or services may vary seasonally, such as swimwear in summer or ski equipment in winter. Agricultural data might exhibit seasonal patterns related to planting and harvesting seasons.\n",
    "\n",
    "6. Economic Factors: Economic conditions can impact seasonal patterns. For example, seasonal fluctuations in employment rates, consumer spending, or tourism can affect various industries and lead to seasonality in corresponding time series.\n",
    "\n",
    "7. Policy and Regulations: Seasonal patterns can arise due to policy changes or regulatory factors. For example, tax seasons, changes in interest rates, or government policies related to subsidies or incentives can influence seasonal components in economic or financial data.\n",
    "\n",
    "8. Demographic Factors: Demographic factors, such as population changes or migration patterns, can introduce seasonality in data. For instance, tourist arrivals might exhibit seasonal patterns due to vacation seasons or cultural events in different regions.\n",
    "\n",
    "It's important to consider these factors and domain-specific knowledge when analyzing time-dependent seasonal components in time series data, as they can provide valuable insights into the underlying causes and help in modeling and forecasting future patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b172f-7813-43f6-9d75-ec6a90413d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc3870-5c4d-4fe8-a800-010d5a864421",
   "metadata": {},
   "outputs": [],
   "source": [
    "Autoregressive (AR) models are commonly used in time series analysis and forecasting to capture the dependence of a variable on its past values. An autoregressive model predicts the future values of a time series based on its own previous values.\n",
    "\n",
    "The general form of an autoregressive model of order p, denoted as AR(p), is as follows:\n",
    "\n",
    "Y(t) = c + φ1 * Y(t-1) + φ2 * Y(t-2) + ... + φp * Y(t-p) + ε(t)\n",
    "\n",
    "In this equation:\n",
    "\n",
    "Y(t) represents the value of the time series at time t.\n",
    "c is a constant term.\n",
    "φ1, φ2, ..., φp are the autoregressive coefficients that determine the impact of previous values on the current value.\n",
    "Y(t-1), Y(t-2), ..., Y(t-p) are the lagged values of the time series.\n",
    "ε(t) is the error term or residual, representing the random noise or unexplained variation.\n",
    "To use an autoregressive model in time series analysis and forecasting, you typically follow these steps:\n",
    "\n",
    "Data Preparation: Ensure your time series data is stationary, meaning it has a constant mean and variance over time. If necessary, perform transformations like differencing or log transformations to achieve stationarity.\n",
    "\n",
    "Model Identification: Determine the appropriate order p for the autoregressive model. This can be done by analyzing the autocorrelation function (ACF) and partial autocorrelation function (PACF) plots of the time series. The significant lags indicate the order of autoregression.\n",
    "\n",
    "Model Estimation: Estimate the autoregressive coefficients (φ1, φ2, ..., φp) using methods like least squares estimation or maximum likelihood estimation. The estimation technique depends on the specific implementation or library you're using.\n",
    "\n",
    "Model Diagnostics: Assess the goodness of fit and diagnostic statistics of the autoregressive model. This involves examining residuals, checking for stationarity, and performing statistical tests such as the Ljung-Box test for autocorrelation in the residuals.\n",
    "\n",
    "Forecasting: Once the autoregressive model is validated, you can use it for forecasting future values. To forecast, provide the model with the previous values of the time series and use the estimated autoregressive coefficients to predict the next values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451fdd50-c81c-42ac-8707-aca6d4281af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load your time series data into a pandas DataFrame\n",
    "# Assuming your data has two columns: 'date' and 'value'\n",
    "data = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Convert the 'date' column to datetime type\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Set the 'date' column as the DataFrame index\n",
    "data.set_index('date', inplace=True)\n",
    "\n",
    "# Create an AR(1) model\n",
    "ar_model = sm.tsa.AR(data['value'])\n",
    "\n",
    "# Fit the AR(1) model\n",
    "ar_result = ar_model.fit(maxlag=1)\n",
    "\n",
    "# Print the estimated parameters\n",
    "print(ar_result.params)\n",
    "\n",
    "# Forecast next 5 values\n",
    "forecast = ar_result.predict(start=len(data), end=len(data)+4)\n",
    "\n",
    "# Print the forecasted values\n",
    "print(forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08bce06-654d-4d29-bad9-dbdafd8ea7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb57f1b1-ed66-4377-811c-c2e3323ba9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fit the Autoregressive Model: First, you need to fit the autoregressive model to your historical time series data. This involves estimating the autoregressive coefficients using methods like least squares estimation or maximum likelihood estimation. The specific approach depends on the library or tool you're using for modeling.\n",
    "\n",
    "Determine the Lag Order: Determine the appropriate lag order (p) for the autoregressive model. This can be done by analyzing the autocorrelation function (ACF) and partial autocorrelation function (PACF) plots of the time series. Significant lags indicate the order of autoregression.\n",
    "\n",
    "Obtain the Lagged Values: To predict future time points, you need to provide the model with the previous values of the time series. Collect the lagged values of the time series up to the lag order (p) that you determined.\n",
    "\n",
    "Make Predictions: Once the model is fitted and you have the lagged values, you can use the estimated autoregressive coefficients to make predictions for future time points. To make a single-step ahead prediction, multiply each lagged value by its corresponding coefficient and sum them up. The sum represents the predicted value for the next time point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a7af96d-db90-422f-9c9e-97a33cd3cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AR:\n",
    "  def __init__(self, p):\n",
    "    self.p = p\n",
    "    self.model = LinearRegression()\n",
    "    self.sigma = None\n",
    "\n",
    "  def generate_train_x(self, X):\n",
    "    n = len(X)\n",
    "    ans = X[:n-self.p]\n",
    "    ans = np.reshape(ans, (-1, 1))\n",
    "    for k in range(1, self.p):\n",
    "      temp = X[k:n-self.p+k]\n",
    "      temp = np.reshape(temp, (-1, 1))\n",
    "      ans = np.hstack((ans, temp))\n",
    "    return ans\n",
    "  \n",
    "  def generate_train_y(self, X):\n",
    "    return X[self.p:]\n",
    "\n",
    "  def fit(self, X):\n",
    "    self.sigma = np.std(X)\n",
    "    train_x = self.generate_train_x(X)\n",
    "    train_y = self.generate_train_y(X)\n",
    "    self.model.fit(train_x, train_y)\n",
    "\n",
    "  def predict(self, X, num_predictions, mc_depth):\n",
    "    X = np.array(X)\n",
    "    ans = np.array([])\n",
    "\n",
    "    for j in range(mc_depth):\n",
    "      ans_temp = []\n",
    "      a = X[-self.p:]\n",
    "\n",
    "      for i in range(num_predictions):\n",
    "        next = self.model.predict(np.reshape(a, (1, -1))) + np.random.normal(loc=0, scale=self.sigma)\n",
    "\n",
    "        ans_temp.append(next)\n",
    "        \n",
    "        a = np.roll(a, -1)\n",
    "        a[-1] = next\n",
    "      \n",
    "      if j==0:\n",
    "        ans = np.array(ans_temp)\n",
    "      \n",
    "      else:\n",
    "        ans += np.array(ans_temp)\n",
    "    \n",
    "    ans /= mc_depth\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c7dcc-db1c-4cb0-abe7-5d6e10f1694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2d3ef-72cd-4d83-b17b-5cebdfae63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A Moving Average (MA) model is a time series model that aims to capture the dependence on the past forecast errors (residuals) rather than the past values of the variable itself. It is different from other time series models such as Autoregressive (AR) and Autoregressive Moving Average (ARMA) models. \n",
    "\n",
    "The key idea behind the MA model is to model the current value of a time series as a linear combination of the past residual errors. The general form of an MA model of order q, denoted as MA(q), is as follows:\n",
    "\n",
    "Y(t) = μ + ε(t) + θ1 * ε(t-1) + θ2 * ε(t-2) + ... + θq * ε(t-q)\n",
    "\n",
    "In this equation:\n",
    "- Y(t) represents the value of the time series at time t.\n",
    "- μ is the constant term or mean of the time series.\n",
    "- ε(t), ε(t-1), ..., ε(t-q) are the past residual errors (forecast errors) at different lags.\n",
    "- θ1, θ2, ..., θq are the parameters (coefficients) that represent the influence of the past residual errors on the current value of the time series.\n",
    "\n",
    "The key differences between MA models and other time series models are:\n",
    "\n",
    "1. Autoregressive (AR) Models: AR models capture the dependence of the current value on its past values. They assume that the current value depends on a linear combination of its previous values. In contrast, MA models assume that the current value depends on the past forecast errors.\n",
    "\n",
    "2. Autoregressive Moving Average (ARMA) Models: ARMA models combine both autoregressive and moving average components. They consider the past values of the time series (autoregressive component) and the past forecast errors (moving average component) to model the current value. ARMA models can capture both short-term dependencies and dependencies due to past forecast errors.\n",
    "\n",
    "3. Autoregressive Integrated Moving Average (ARIMA) Models: ARIMA models include an additional differencing step to achieve stationarity before applying the ARMA modeling. The differencing helps remove trends and seasonality from the time series.\n",
    "\n",
    "MA models are useful for modeling time series data that exhibit persistence in the forecast errors or have a dependence on the past forecast errors rather than the past values. They can be combined with other models (such as AR or ARMA) to form more complex models like Autoregressive Moving Average (ARMA) or Autoregressive Integrated Moving Average (ARIMA) models, depending on the characteristics of the time series and the specific modeling requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216a8a45-3a02-460d-9661-dff6dd3b3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a5ecb-619e-4ea5-8f46-b2ef1aa57380",
   "metadata": {},
   "outputs": [],
   "source": [
    "A mixed Autoregressive Moving Average (ARMA) model, also known as an ARMA(p, q) model, combines both autoregressive (AR) and moving average (MA) components to capture the dependence on both the past values and the past forecast errors of a time series. It differs from pure AR or MA models in that it incorporates both components simultaneously.\n",
    "\n",
    "The general form of an ARMA(p, q) model is as follows:\n",
    "\n",
    "Y(t) = c + φ1 * Y(t-1) + φ2 * Y(t-2) + ... + φp * Y(t-p) + θ1 * ε(t-1) + θ2 * ε(t-2) + ... + θq * ε(t-q) + ε(t)\n",
    "\n",
    "In this equation:\n",
    "- Y(t) represents the value of the time series at time t.\n",
    "- c is a constant term.\n",
    "- φ1, φ2, ..., φp are the autoregressive coefficients that determine the impact of previous values on the current value.\n",
    "- θ1, θ2, ..., θq are the moving average coefficients that determine the impact of past forecast errors on the current value.\n",
    "- ε(t), ε(t-1), ..., ε(t-q) are the past forecast errors at different lags.\n",
    "- ε(t) is the error term or residual, representing the random noise or unexplained variation.\n",
    "\n",
    "The key differences between mixed ARMA models and pure AR or MA models are as follows:\n",
    "\n",
    "1. AR Models: AR models capture the dependence of the current value on its own past values. They assume that the current value depends on a linear combination of its previous values.\n",
    "\n",
    "2. MA Models: MA models capture the dependence of the current value on past forecast errors. They assume that the current value depends on a linear combination of the past forecast errors.\n",
    "\n",
    "3. ARMA Models: ARMA models combine both AR and MA components. They consider the past values of the time series (AR component) and the past forecast errors (MA component) to model the current value. ARMA models can capture both short-term dependencies and dependencies due to past forecast errors.\n",
    "\n",
    "Mixed ARMA models provide a more comprehensive framework for modeling time series data that exhibit both autoregressive and moving average properties. They allow for capturing different types of dependencies in the data and can be useful for forecasting, anomaly detection, and understanding the underlying dynamics of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8248409c-4920-42fe-a47c-9f8ab3b9df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "# load dataset\n",
    "def parser(x):\n",
    " return datetime.strptime('190'+x, '%Y-%m')\n",
    "series = read_csv('shampoo-sales.csv', header=0, index_col=0, parse_dates=True, squeeze=True, date_parser=parser)\n",
    "series.index = series.index.to_period('M')\n",
    "# split into train and test sets\n",
    "X = series.values\n",
    "size = int(len(X) * 0.66)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "# walk-forward validation\n",
    "for t in range(len(test)):\n",
    " model = ARIMA(history, order=(5,1,0))\n",
    " model_fit = model.fit()\n",
    " output = model_fit.forecast()\n",
    " yhat = output[0]\n",
    " predictions.append(yhat)\n",
    " obs = test[t]\n",
    " history.append(obs)\n",
    " print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "# evaluate forecasts\n",
    "rmse = sqrt(mean_squared_error(test, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "# plot forecasts against actual outcomes\n",
    "pyplot.plot(test)\n",
    "pyplot.plot(predictions, color='red')\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
